{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271c4bb-85d3-45d1-aae3-bc7ce374f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f4e91-c407-4534-9893-9e1762208e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(tiktoken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47356449-4302-414d-8b70-bfe5382d7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktoken.list_encoding_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03afd9-df63-45d1-ab5d-ca19a7b4b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7950d47-0dd3-493b-8305-9229022cfbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"Let's test tokenizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee6466-a170-42b1-a119-2ecda9476d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9e368-33a0-4564-8d8d-e59a1ad4ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_line = 20\n",
    "total_lines = int(tokenizer.max_token_value / tokens_per_line)\n",
    "column_width = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa59d5-8f81-44c8-b12b-fc7ac884567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, total_lines):\n",
    "    token_ids = [(tokens_per_line * i ) + id for id in range(0,tokens_per_line)]\n",
    "    line = '|'.join(f\"{token_id:>{column_width}}\" for token_id in token_ids)\n",
    "    print(line)\n",
    "    \n",
    "    tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
    "    line = '|'.join(f\"{token:>{column_width}}\" for token in tokens)\n",
    "    print(line, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c414cfc-a0dd-4ebd-b541-5c31ab2050fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([628])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143073ff-ca7b-47aa-ac76-c70e6475abab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
